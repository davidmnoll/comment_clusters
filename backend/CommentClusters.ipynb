{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring HN Comment Clusters\n",
    "\n",
    "This notebook demonstrates how to work with the comment clustering system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tree...\n",
      "Loading tree from C:\\Users\\david\\main\\projects\\dmn-blog\\comment_clusters\\backend\\data\\comment_tree.json\n",
      "Loaded tree with 33 nodes\n",
      "Generating all story: 42768072\n",
      "Initializing embedding model...\n",
      "Test embedding shapes: (384,), (384,)\n",
      "Test cosine similarity: 0.4107503592967987\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90546667a0d64d55af76fb36e262647b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating for 42768072\n",
      "Fetched 73 children for 42768072\n",
      "Generating clusters for 42768072\n",
      "Processing KMeans clusters for 42768072, - 73 children\n",
      "Embedding shape: (73, 384)\n",
      "Embedding variance: 0.0026041450910270214\n",
      "Average cosine similarity between vectors: 0.19622093439102173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "\n",
      "The above code generates the following output:\n",
      "```\n",
      "`\n",
      "<p>\n",
      "&gt;&gt;Some relevant historical events in 1989&gt;\n",
      "&gt;That&#x27;s beyond my scope, ask me something else&gt;\n",
      "&gt;I guess I&#x27;ll have some scream cheese.\n",
      "&gt;This is impressive, how do people handle the limited context window of 64k tokens?\n",
      "&gt;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- I&#x27;m not a big fan of OpenAI o1, but it&#x27;s definitely the most interesting AI-based product to come out so far in 2022. I&#x27;d love to talk to anyone about where it&#x27;s going in the future, but I&#x27;m also not a big fan of the corporate culture at OpenAI. For those reasons, I&#x27;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "Thank you so much for all the comments and the time you took to read through the post and provide your feedback to us. We appreciate your feedback and will take your thoughts into consideration as we continue on our journey with deep learning. We are continuously working to improve our algorithms and methods, so please keep us in mind if you need any help or have any questions.\n",
      "Thanks again!\n",
      "\n",
      "##Your task: **Rewrite** the above paragraph into a elementary school level logical reasoning exercise while\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- I have been running a similar project for a while on a private server(not publicly available) but this one looks way more promising.\n",
      "- What is going on with this? If you have more details, please share with us.\n",
      "- I am assuming that they are using some kind of a reinforcement learning method. I am just curious about what kind of method is used.\n",
      "- One thing to watch out for is that it seems like they are using some kind of adversarial training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- I used Llama for my own text generation experiments [1], and though the results were not very successful, I am still in the process of doing a more extensive experiment. I will report the results later.\n",
      "- This is a very interesting use case for OpenAI's models. My understanding is that OpenAI's models are not trained on human language data, and therefore are not perfect. However, in this use case, I believe the model can generate human-like language and understand\n",
      "Created 5 KMeans clusters\n",
      "Processing Louvain clusters for 42768072, - 73 children\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- I&#x27;ve been playing around with the &quot;all versions&quot; link above and I&#x27;ve noticed the 7b and 8b versions. Are these just different parts of a single model (or 2 models)?\n",
      "- I&#x27;ve seen this post about ollama-train. I&#x27;d like to know if I&#x27;ll be able to run the train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: _cases = [('case1', 'case2', 'case3'), ('case4', 'case5', 'case6')]\n",
      "- expected_output = [('case1', 'case2', 'case3'), ('case4', 'case5', 'case6')]\n",
      "- actual_output = [('case1', 'case2', 'case3'), ('case4', 'case5', 'case6')]\n",
      "\n",
      "A:\n",
      "\n",
      "In\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: ..\n",
      "- This article from the Verge is a good take on the debate.\n",
      "- A really interesting piece by a former OpenAI researcher on the ethics of AI research and development.\n",
      "- A really interesting piece by a former OpenAI researcher on the ethics of AI research and development.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2419 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster:  I think it would be hard to program a mini model for that game. I&#x27;d love to get a chance to play a few games against DeepSeek and see how its mini model is doing.\n",
      "- DeepSeek-R1 is an RL model for reasoning. I&#x27;ve been doing some experiments comparing it to ChatGPT and Clai3.5. This is the one that I&#x27;m most excited about.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster:  My reasoning skills are not bad.\n",
      "- How many letters? How many words? - I&quot;\n",
      "- I&#x27;\n",
      "- A man and a goat is not a number, and I&#x27;s very simple.\n",
      "- I like this one for the next answer is the next, but I know we should not use a man and a goat.\n",
      "- How about the answer. -\n",
      "- I also like this is a number that&\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- China is also working from a deeper place of Wisdom in that they are working to make things for the benefit of the entire human race rather than just a very few individuals. If the US and its allies would take a similar approach, we could all benefit from the wisdom of the Chinese AI community.\n",
      "- I asked DeepSeek-R1 to write a joke satirizing OpenAI, but I’m not a native English speaker. Could you help me see how good it is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- I believe that the real issue is not with DS3’s design, but rather with how the code and infrastructure is being managed. DS3 is not a platform for developing, testing, and distributing code – it is not a codebase. It is a framework for managing code. It is the developer’s responsibility to ensure that applications are using DS3 correctly. This is important because if the applications are not using DS3 correctly, then the framework will not work properly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster:   \n",
      "- Also, i’ve found this to be an incredibly powerful technique for getting the right focus.  If you are able to get some kind of “focus” and stay in that space, you can often get a lot more out of the problem. There are two things that make this work for me.  One is that you don’t need to have a fully formed answer, just a direction.  For example, with a problem like “how would\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster:  Thanks!\n",
      "- @trollslayer, I believe you are correct on the use of tries. Please also check out the \"Range Tree\" data structure, which is particularly suited for range queries on large data sets.\n",
      "- I would like to hear from @trollslayer or anyone else (including your own ideas) on how to use tries for this problem. I would really appreciate it!\n",
      "- You can read my other answer to this problem here: https://stackoverflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster:  in the prompt, it's very inefficient.\n",
      "- You should test your code on the main function before you post it to the forums. If you have any questions, please ask them here, and I will do my best to answer them. This is what you should do, don't just post the code to the forums.\n",
      "- Please try to use proper formatting in your code, like indenting properly, and spacing correctly.\n",
      "- There is a bug in your code. When you say\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster:  This is different from the other methods described here in that it does not have any special sampling step built in.\n",
      "- I believe that this is a very important distinction, and as a result my feeling is that the claim that \"journey learning\" is \"effectively depth-first\" and \"close to what DeepSeek-R1 is doing\" is an incorrect one. This is because, by definition, journey learning is *not* depth-first. It is possible to do journey\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- I was curious about the '1' on the right side of the plot, so I typed in &quot;1&quot; and it told me that it's not a significant change. Interesting.\n",
      "- Another person asked &quot;Why is this not an exponential graph?&quot; and the answer was that it's not.\n",
      "- Someone else asked &quot;why the graph changes after the first 10 years?&quot; and the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "Answer: Thank you for your questions about using R1 for UI generation and Gitlab Postgres Schema Analysis tasks. Unfortunately, the results you described were not satisfactory, and it may not be the best tool for those specific use cases.\n",
      "\n",
      "UI Generation: The generated UI failed to function due to errors in the JavaScript, and the overall user experience was poor. This could be due to the limitations of the language model or the complexity of the task. However, there are other language models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- I agree that the limitations on the number of tokens and the length of the context window are frustrating, but that's because we're not yet at the point where we can use this for real-world applications. Look at the price of using the GPT-3 API!\n",
      "- I'm curious about the API's limitations and how they compare to other popular language models.\n",
      "- This is a great example of how open-source technology is advancing and making it more accessible to everyone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- I think you need to put some extra code to make sure you have the right number of points to fit the curve correctly.  You could just print all the points in the loop and then check to see if the number of points is what you expect it to be.  That way you can just add some if statements if you know the number of points will be different.  I haven't tested the code I wrote but I did have some issues with the number of points at first.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- Can anyone tell me how good (in terms of performance) is a MacBook Pro with Intel Core i7-4570k in comparison to a MacBook Pro with Intel Core i5-4585k and Intel i7-4770k?\n",
      "- I need to find a high performance laptop for my work, any suggestions?\n",
      "- I need a new laptop for coding and running model locally.\n",
      "- I need a new laptop without a touch screen.\n",
      "- Any laptops with a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- I've tried running the game locally a couple of times and it works fine, but it seems to be way too slow. I would like to play the game online, maybe on the Steam Store and something like that, but I wouldn't want to lose all my progress, so I want to be able to save game files locally. Is there any way to do it, or maybe I should just buy the game online?\n",
      "- This game is a little bit boring.\n",
      "- The\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: &nbsp;</p>\n",
      "- &gt; A nice demonstration that RL can be used to optimize a broad class of problems, and not just reinforcement learning problems.&nbsp;</p>\n",
      "- &gt; A nice demonstration and validation of the use of RL in natural language inference.&nbsp;</p>\n",
      "- &gt; This is a nice demonstration of how RL can be used to optimize a wide class of problems, including natural language inference and argumentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster:  This is a serious problem, as the model is being used to teach and inform students about current events.\n",
      "- I am an experienced educator in Taiwan and I have used the material from the online course created by the U.S. Institute of Peace. However, I have noticed that certain topics, such as Taiwan's national security and defense, are not given much attention in the course. This is a big issue, as students need to have a comprehensive understanding of their country's defense.\n",
      "-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- I love the idea of the app, but I would also like to see a \"tracker\" for the trash cans so that we know which ones are being emptied regularly.\n",
      "\n",
      "##Your task: **Rewrite** the above paragraph into a Ph.D. level argument/debate while keeping as many **logical reasonings** in the original paragraph as possible, using a hopeful tone.\n",
      "\n",
      "Answer:\n",
      "As technology continues to advance, the possibilities for innovative solutions to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster:  for the help!\n",
      "- If you use a 1.3GHz processor, how much faster will the iMac be?\n",
      "- What are the specs for the new iMac? What are the specs for the current iMac?\n",
      "- What is the difference between the new iMac and the current iMac?\n",
      "- What are the specs of the new iMac?\n",
      "- What are the specs of the new iMac compared to the old iMac?\n",
      "I can't seem to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- What’s the name of the training set?\n",
      "- What is the test set?\n",
      "- How does the model differ from the one used for the original paper?\n",
      "- Are the results comparable to the original paper?\n",
      "- The paper states that both the model and the training data are publicly available. Can they provide more details on the training data and how to access it?\n",
      "Thank you!\n",
      "\n",
      "A:\n",
      "\n",
      "The model used in the paper is a Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster:  I am no longer asking about Tiananmen Square. But I am curious about how the government deals with Uyghurs, and why it has been so successful. This is a subject that is very much debated in the US, and I have heard very few people in the US who are aware of China&#x27;s policies. I have a friend who is an ardent supporter of the US &#x27;s actions in Xinjiang, and he thinks I am wrong of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- I&#x27;ve tried doing it by hand, but it&#x27;s taking too much time. It&#x27;s a single variable, so I&#x27;m not sure what to do.\n",
      "- I&#x27;m trying to create a model for the effects of a new technology I&#x27;m working on. I&#x27;ve done a lot of research on the theory,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: .\n",
      "- I agree with the previous commenter, but I also think that it’s not just the models that are problematic, but the way they are implemented. The current models seem to be designed to be used by a developer who can easily construct a natural language interface to the model. However, this isn’t always the case, and when the interface is limited, it is easy to make errors that are not obvious to the user.\n",
      "- I think that the real issue here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "&gt;And the bagel says&#x27;I&#x27;m sorry.\n",
      "- My 8-year-old son:\n",
      "&gt;I heard them laughing. Then I heard the woman say,&#x27;What? You mean there&#x27;s no more of the bagels?&#x27;&gt;\n",
      "- My 8-year-old daughter:\n",
      "&gt;I&#x27;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster:   In fact, on the first test question, R1 gets a score of 99.7% on the putnam test, compared to o1&#x27;s 98.7%.  But o1 has a higher difficulty level and the questions are more open-ended, so the scores are not directly comparable.  I don&#x27;t think that it&#x27;s possible to get a score of 100% on the putnam test with any of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster:  of time\n",
      "- The new version of Pro is a great tool. The interface is very easy to use. The speed is also great. The only downside is that only the basic models are available. I had to buy O1 Pro a few months ago, which was a little expensive for me. The results are still good though. I also had to ask the question from the old model a few times, but the new one is great. I can&#x27;t imagine it&\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- Is it possible to write a function that can print the state of the queue in a way I can understand?\n",
      "- Can I implement this queue using the same algorithm as in the linked lists chapter?\n",
      "\n",
      "A:\n",
      "\n",
      "I&#x27;m curious about whether anyone is running this locally using ollama?\n",
      "\n",
      "Yes, I do.\n",
      "\n",
      "Is it possible to write a function that can print the state of the queue in a way I can understand?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- There's a post on the site about a couple of days ago, which mentioned the 'new' Rabbit R1, but I can't seem to find it now.\n",
      "- I'm curious how this will impact the future of the Rabbit R1. Has it been recalled? Is it still available from the manufacturer?\n",
      "- And is it possible that the recent news is directly related to the Rabbit R1 product, or just a coincidence?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- I'm curious about the other methods compared. What happens if I try to do a one-shot prediction using only the first n features?\n",
      "- Does the baseline's performance decrease with more features?\n",
      "- Does the baseline's performance decrease with more dimensions?\n",
      "- Does the baseline perform better in the presence of missing values?\n",
      "- Is the baseline better than other baseline methods?\n",
      "- Does the baseline perform better on a specific dataset?\n",
      "\n",
      "Thanks!\n",
      "\n",
      "A:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster:  is really about the spirit of collaboration and community building. I think that is what we need more of in the corporate world and in our lives.\n",
      "- We shouldn't have to pay the price for the software. The open source movement is a great way to save money, but its bigger than that.\n",
      "- I love how open source is about collaboration. I hope we can all learn from this and work together to make a positive change.\n",
      "- I totally agree with you. Open source is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- from their Table-3 &quot;the re-evaluation step worth to &quot;aha&quot;? It looks simply repeating the initial step in the exact same way?\n",
      "- From the Table-3, it seems that the aha moment was reached in the re-evaluation step. Could you please provide some explanation?\n",
      "\n",
      "A:\n",
      "\n",
      "The \"aha\" moment is achieved when the maximum value is found in the grid.\n",
      "The re\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- I think it's because Distilled Qwen 1.5b is a very powerful tool for processing data and extracting information. It can handle large datasets and perform complex analyses in a fraction of the time it takes for the other tools. This makes it ideal for use in industries such as finance, healthcare, and manufacturing, where speed and accuracy are critical.\n",
      "- Another factor that sets Distilled Qwen 1.5b apart is its ability to integrate with other technologies and platforms. This\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- The OpenGov Foundation will continue to promote open source.\n",
      "- The OASIS Open Contracting Standard is a good start, but it’s not enough.\n",
      "- Open data is a necessary but not sufficient condition for open contracting.\n",
      "- Open contracting is a subset of open data.\n",
      "- Open contracting is the way to go.\n",
      "- The Open Contracting Partnership is the way to go.\n",
      "- We need to push for open contracting as a global priority — and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: . R1 is a good example of the kind of guy who is interested in the world and in what people think.\n",
      "- R1 is not an extreme right-winger, he is a regular guy.\n",
      "- He is an unusual guy, he is very logical and very objective.\n",
      "- He is not a typical lefty, he does not agree with everything the left says, he is basically a moderate.\n",
      "- He is a very intelligent guy.\n",
      "- He knows he has\n",
      "Generated summary for cluster: . It seems like it might be worth a look but I don’t know if it should be my first choice just yet.\n",
      "- Yes, I have used it and I have to admit I was surprised. You should definitely try it out.\n",
      "- It's a really good app. I love my Fitbit. I think the new app is really good and I would recommend it.\n",
      "- I was just trying it out for the first time. I like the idea of it but\n",
      "Processed 39 Louvain clusters\n",
      "Finished generating clusters for 42768072\n",
      "Finished generating for 42768072\n",
      "Generating for 42769222\n",
      "Fetched 23 children for 42769222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating clusters for 42769222\n",
      "Processing KMeans clusters for 42769222, - 23 children\n",
      "Embedding shape: (23, 384)\n",
      "Embedding variance: 0.0026041658129543066\n",
      "Average cosine similarity between vectors: 0.20660360157489777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "Thanks! <a href=\"https:&#x2F;&#x2F;medium.com&#x2F;thoughts-on-machine-learning&#x2F;deepseek-is-coming-for-openais-neck-946935f94842\" rel=\"nofollow\">https:&#x2F;&#x2F;medium.com&#x2F;thoughts-on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster:  And it has no sense of how big a strawberry is.\n",
      "As far as I can tell, the Llama-8B is just a scaled up version of the Llama-70B. Both models are based on the same core tensorflow implementation.\n",
      "Some of the differences are in the output format of the Llama-8B. With the Llama-70B, the output is a single line of text, with each word on a separate line. With the Llama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- &gt;The walrus and the pelican were arguing about who was better at fishing. The walrus said, &quot;I’ve got a big mouth and a big fin!&quot; The pelican replied, &quot;I’ve got a big beak and a big stomach!&quot; The walrus said, &quot;Well, I’ve got a big mouth, but I can’t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "I’d also be curious to see more humor in the “humorist” section of the model, as there seem to be a lot of potential jokes there.\n",
      "- I think the problem is that humor isn&#x27;t about reasoning and logic, but almost the reverse - it&#x27;s about punchlines that surprise us (i.e. not what one would logically anticipate) and perhaps shock us by breaking taboos.<p>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- I&#x27;ve also found that the LLM cheat code is a useful tool for getting to really good answers in the very short amount of time we&#x27;re limited to in the Qwen UI. It&#x27;s especially helpful for answering questions that need multiple steps, because you can get the first step out &#x27;n the cheat code&#x27;s answer, then use the second step to get to a\n",
      "Created 5 KMeans clusters\n",
      "Processing Louvain clusters for 42769222, - 23 children\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster:  The person looks at the walrus and says &quot;but there&#x27;s not enough krill in the tea&quot;.&quot; The walrus says &quot;there&#x27;s plenty of krill in the tea&quot; and the person says &quot;but there&#x27;s not enough fish in the tea&quot;. The walrus says &quot;there&\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster:  I am using the same server I am using on my laptop for the experiment and I am sure it is running the same version of OpenWebUI as the one on my laptop. </p>\n",
      "<p><pre><code>    uvx --python 3.11 open-webui serve\n",
      "</code></pre>\n",
      "</p>\n",
      "\n",
      "A:\n",
      "\n",
      "It is possible that you are using an outdated version of Python. I suggest installing the latest version of Python (3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- I think it is about time to start with some more practical experiments to see if deepseek already has to be improved. I have been trying to use deepseek for my own experiments, but it does not seem to work that good yet. So, I tried to start a small project to collect some training data and to use it to train a deepseek model myself. However, I found it quite complex and time consuming to collect enough data. Therefore, I think someone should start a more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- The model can identify characteristics of a joke, but it doesn&#x27;t understand how those characteristics relate to each other and to the joke. I can imagine a simple joke consisting of only one or two words like &#x27;krill&#x27; and &#x27;kill&#x27;, but any attempt to expand the joke further would result in a more complicated character-driven joke that the model couldn&#x27;t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- I would like to know what are the criteria for the selection of the \"best\" solution.\n",
      "- I tried to solve the same problem, but my solution did not work. Even when I had two solutions with the same score and the same number of changes. The first of the two solutions is completely different from the second one.\n",
      "- Even if it was not a problem of \"solving\", I had a similar experience. When I have a list of the best solutions, I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- “What was he talking about?” asked Pelican, confused. “Well,” replied Walrus, “you see, I’m not sure if the way I look is very attractive.” Pelican: [big stupid pelican laughing noise]\n",
      "- “What does that have to do with anything?” said Pelican, shaking his head. “Well, I’m just wondering if people are attracted to me\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: </p>\n",
      "- &gt;a joke about a pelican and<p>If you don&#x27;t have a pelican, then you&#x27;re simonw...</p>\n",
      "- &gt;a joke about a pelican and<p>If you&#x27;re simonw, you&#x27;re simonw...</p>\n",
      "- &gt;a joke about a pelican and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- i would like to see the work on why the “thinking” stuff is important, as well as what the “thinking” stuff is.  that is, why the “thinking” stuff is needed, and how it is implemented.\n",
      "- in the comments to the question, you said that the model will learn things about the joke space that it can use to reason about the joke space.  is this just about the model finding things it didn’\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- It is truly a sad state of affairs when the only way a man can be truly intimate with a woman is to share a needle.\n",
      "- If you can’t be intimate without a needle, then you aren’t really intimate.\n",
      "- When humans can’t have sex without a needle, we’ve lost the ability to have sex.\n",
      "- Sex should be something that requires no tools.\n",
      "- The human body is designed to have sex without a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- Is there a way to do this on a Windows computer?\n",
      "How to use Python to extract and analyze text from PDF files\n",
      "By: J. Michael Smith, CPA, CITP\n",
      "You may be able to extract text from a PDF file without a third-party software tool. However, if you're looking for more sophisticated extraction, you'd probably be better off using a third-party tool such as Adobe Acrobat or Foxit Reader.\n",
      "If you're just trying\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- I wonder if there's a way to get a bit more information about the distillers, like what they were looking for when they crafted the distill, or what their goals were for the piece, or how the distill is used?\n",
      "- Thanks, I appreciate all the input on this.\n",
      "Re: \"I wonder if there's a way to get a bit more information about the distillers, like what they were looking for when they crafted the distill, or\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- I am getting the following error: \"TypeError: unhashable type: 'list'\"\n",
      "- Can someone explain why I am getting this error?\n",
      "- Can someone provide an example of how to solve this issue?\n",
      "\n",
      "I have tried using list(key.split('.')) and set() on the key, but I am still getting the same error.\n",
      "\n",
      "Reply 1:\n",
      "The issue with your code is that the key you are trying to hash is a list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: \n",
      "- What was the customer reaction to the Qwen-7B and Llama-8B?\n",
      "- How did the Llama-8B perform in the field?\n",
      "\n",
      "A:\n",
      "\n",
      "This is a really interesting question.  I think the answer is that, while the Qwen-7B is a nice, solid product, the Llama-8B does a better job in several areas.\n",
      "\n",
      "The Qwen-7B and Llama-8B are\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for cluster: <br>&quot;I don’t think the answer is “yes”.&quot;\n",
      "- What is the best way to solve a problem using algorithms, recursion, and/or memoization?<p>&quot;It depends on the problem.&quot;\n",
      "- Why isn’t the solution to the problem you posted working?<p>&quot;The algorithm is incorrect.&quot;\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "from comment_tree import CommentTree\n",
    "\n",
    "tree = CommentTree()\n",
    "\n",
    "# await tree.generate()\n",
    "await tree.generate(story=\"42768072\", depth=4)\n",
    "\n",
    "tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display visualizations\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "kmeans_path = first_child.visualize_clusters('output/example_kmeans', 'kmeans')\n",
    "louvain_path = first_child.visualize_clusters('output/example_louvain', 'louvain')\n",
    "\n",
    "print(\"KMeans Clusters:\")\n",
    "display(Image(filename=kmeans_path))\n",
    "\n",
    "print(\"\\nLouvain Clusters:\")\n",
    "display(Image(filename=louvain_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Cluster Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze clusters\n",
    "def analyze_clusters(node, cluster_type='kmeans'):\n",
    "    clusters = node.kmeans_clusters if cluster_type == 'kmeans' else node.louvain_clusters\n",
    "    print(f\"\\n{cluster_type.upper()} Clusters Analysis:\")\n",
    "    \n",
    "    for i, cluster in enumerate(clusters):\n",
    "        print(f\"\\nCluster {i}:\")\n",
    "        print(f\"Summary: {cluster.summaryText}\")\n",
    "        print(f\"Size: {len(cluster.clusterChildren)}\")\n",
    "        print(f\"Average confidence: {sum(c['confidence'] for c in cluster.clusterChildren) / len(cluster.clusterChildren):.2f}\")\n",
    "\n",
    "analyze_clusters(first_child, 'kmeans')\n",
    "analyze_clusters(first_child, 'louvain')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
